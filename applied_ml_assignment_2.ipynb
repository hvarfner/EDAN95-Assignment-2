{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Session - Probability Distributions and Basic ML Concepts\n",
    "---\n",
    "\n",
    "In this lab session, we are going to consider various probability distributions, conjugacy, Maximum Likelihood and overfitting, as covered in lecture 2 and 3.\n",
    "\n",
    "__Note__: You can complete the lab entering code only in the  __# EXPECTED SPACE FOR STUDENT CODE__-blocks as well as the empty cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Can be installed through \"pip install numpy scipy pandas matplotlib\"\n",
    "import time\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from scipy.stats import norm, beta \n",
    "from scipy.linalg import det, inv\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import plot_gaussian, show_images, show_marginals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 1: Gaussian Distributions\n",
    "\n",
    "We will start by considering the most prevalent distribution - the Gaussian. As stated in Lecture 2 (Section on Gaussians) the Gaussian has several appealing properties that makes it easy to work with. In the following tasks, we will highlight some of these.\n",
    "\n",
    "**Note:** For the following tasks, the purpose is to understand the properties of Gaussian distributions. As such, any readly-made distributions (scipy.stats.norm etc.) are disallowed. Using NumPy or SciPy for Linear Algebra is permitted.\n",
    "\n",
    "Below, you see an implementation of a Gaussian distribution. We will use this highlight and implement properties that are typical of the Gaussian distribution. Please read through, and note the dunder (double under, \\_\\__methodname_\\__) methods that are implemented. These allow us to add and subtract one Gaussian with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDistribution:\n",
    "    \n",
    "    def __init__(self, mean, cov):\n",
    "        \"\"\"\n",
    "        A Gaussian Distribution in arbitrary number of dimensions\n",
    "\n",
    "        Args:\n",
    "            mean (np.ndarray): Mean of the distribution.\n",
    "            cov (np.ndarray): Covariance of the distribution.\n",
    "        \"\"\"\n",
    "        self.mean = np.array(mean).reshape(1, -1)\n",
    "        self.cov = np.array(cov)\n",
    "        self.dims = self.mean.shape[1]\n",
    "        \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Generates a string representation of the object, used for plotting\n",
    "        and printing\n",
    "\n",
    "        Returns:\n",
    "            str: String representation of the object\n",
    "        \"\"\"        \n",
    "        if self.dims == 2:\n",
    "            string = f'Gaussian, Dim: {self.dims}    Mean: {self.mean}    Covariance: {self.cov[0, :]}{self.cov[1, :]}'\n",
    "            \n",
    "        else:\n",
    "            string = f'Gaussian, Dim: {self.dims}    Mean: {np.round(self.mean[0, 0], 6)}    Covariance: {self.cov}'\n",
    "        return string.replace('][', '     ').replace('[', '  ').replace(']', '  ')\n",
    "        \n",
    "    \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Adds two GaussianDistributions (or GaussianDistribution + scalar)\n",
    "        together, assuming zero cross-covariance.\n",
    "\n",
    "        Args:\n",
    "            other (GaussianDistribution): Another GaussianDistribution\n",
    "            of the same dimensionality\n",
    "\n",
    "        Returns:\n",
    "            GaussianDistribution: The resulting distribution\n",
    "        \"\"\"\n",
    "        if not isinstance(other, type(self)):\n",
    "            other = np.array(other)\n",
    "            mean = self.mean + other\n",
    "            cov = self.cov\n",
    "        \n",
    "        else:\n",
    "            mean = self.mean + other.mean\n",
    "            cov = self.cov + other.cov\n",
    "        \n",
    "        return GaussianDistribution(mean, cov)\n",
    "              \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        Subtracts two GaussianDistributions (or GaussianDistribution + scalar)\n",
    "        together, assuming zero cross-covariance.\n",
    "\n",
    "        Args:\n",
    "            other (GaussianDistribution): Another GaussianDistribution\n",
    "            of the same dimensionality\n",
    "\n",
    "        Returns:\n",
    "            GaussianDistribution: The resulting distribution\n",
    "        \"\"\"\n",
    "        if not isinstance(other, type(self)):\n",
    "            other = np.array(other)\n",
    "            mean = self.mean - other\n",
    "            cov = self.cov\n",
    "            \n",
    "        else:\n",
    "            mean = self.mean - other.mean\n",
    "            cov = self.cov + other.cov\n",
    "        \n",
    "        return GaussianDistribution(mean, cov)  \n",
    "           \n",
    "    # sampling is a bit intricate - thus, we cheat a little bit here\n",
    "    def sample(self, n_samples=1):\n",
    "        \"\"\"\n",
    "        Samples from the distribution.\n",
    "\n",
    "        Args:\n",
    "            n_samples (int, optional): The number of samples to generate.\n",
    "            Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A numpy array of the sampled points.\n",
    "        \"\"\"        \n",
    "        return norm(self.mean, self.cov).rvs(n_samples)\n",
    "            \n",
    "    # probability density function - general form\n",
    "    def pdf(self, points):\n",
    "        '''\n",
    "        Evaluates the probability density function of the distribution.\n",
    "\n",
    "        Args:\n",
    "            points (np.ndarray): the points we want the probability density of,\n",
    "            can come in either a list, np.array([dims]) or np.array([samples, dims])\n",
    "            format\n",
    "        \n",
    "        Returns: \n",
    "            np.ndarray: A numpy array of probability densities\n",
    "        '''\n",
    "\n",
    "        points = np.array(points)\n",
    "        if len(points.shape) == 1:\n",
    "            points = points.reshape(1, -1)\n",
    "            \n",
    "        offset = points - self.mean\n",
    "        if self.dims == 1:\n",
    "            norm_const = 1.0 / (np.power(2*np.pi, float(self.dims)/2) * np.sqrt(self.cov))\n",
    "            exponent = np.exp(-0.5 * np.power(offset, 2) / self.cov)\n",
    "            return norm_const * exponent\n",
    "            \n",
    "        else:\n",
    "            norm_const = 1.0 / (np.power(2*np.pi, float(self.dims)/2) * np.sqrt(det(self.cov)))\n",
    "            exponent = np.exp(-0.5 * offset.dot((inv(self.cov)).dot(offset.T)))\n",
    "            return np.diag(exponent * norm_const)\n",
    "    \n",
    "    \n",
    "    def get_conditional(self, dim, value):\n",
    "        '''\n",
    "        Computes the conditional distribution on the remaining dimensions given\n",
    "        the conditioned value in dimension dim.\n",
    "\n",
    "        Args:\n",
    "            dim: int - the dimension (assume only one) which we condition on\n",
    "            value: float - the value of the conditioned dimension\n",
    "            \n",
    "        Returns: \n",
    "            GaussianDistribution: a new distribution of dimension dim - 1\n",
    "        '''\n",
    "            \n",
    "        # EXPECTED SPACE FOR STUDENT CODE - START\n",
    "        # some hints to help\n",
    "        non_cond_dims = np.arange(self.dims) != dim\n",
    "        cond_dims = np.arange(self.dims) == dim\n",
    "        \n",
    "        cov_xy = self.cov[non_cond_dims, cond_dims][np.newaxis, :]\n",
    "        \n",
    "        # EXPECTED SPACE FOR STUDENT CODE - END\n",
    "        \n",
    "        return GaussianDistribution(conditional_mean, conditional_cov)\n",
    "\n",
    "    def marginalize(self, dim):\n",
    "        '''\n",
    "        Computes the marginal distribution on the remaining dimensions when having\n",
    "        marginalized out dimension dim.\n",
    "        \n",
    "        Args:\n",
    "            dim: int - the dimension (assume only one) which we marginalize\n",
    "            \n",
    "        Returns: \n",
    "            GaussianDistribution: a new distribution of dimension dim - 1\n",
    "        '''\n",
    "            \n",
    "        # EXPECTED SPACE FOR STUDENT CODE - START\n",
    "        \n",
    "        # EXPECTED SPACE FOR STUDENT CODE - END\n",
    "        \n",
    "        return GaussianDistribution(marginal_mean, marginal_cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "We first want to demonstrate how a Gaussian looks and how it is expected to behave - particularly its linear properties. __Throughout this task, we assume that the random variables we work with are independent of one another__ (e.g. __A__ is independent of __B__). Create four Gaussians:\n",
    "\n",
    "__A__: $\\mu = 1, \\sigma^2 = 2$\n",
    "\n",
    "__B__: $\\mu = -2, \\sigma^2 = 0.1$\n",
    "\n",
    "__C__: $\\mu = \\begin{bmatrix}\n",
    "1 & 0\n",
    "\\end{bmatrix}, \\quad\\Sigma^2 = \\begin{bmatrix}\n",
    "1 & 0.9\\\\\n",
    "0.9 & 2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "__D__: $\\mu = \\begin{bmatrix}\n",
    "-1 & 2\n",
    "\\end{bmatrix}, \\quad\\Sigma^2 = \\begin{bmatrix}\n",
    "1 & -0.7\\\\\n",
    "-0.7 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Visualize, through the provided plotting function below, _plot_gaussian_:\n",
    "\n",
    "a. __A__\n",
    "\n",
    "b. __B__\n",
    "\n",
    "c. __A__ + __B__\n",
    "\n",
    "d. __C__\n",
    "\n",
    "e. __D__\n",
    "\n",
    "f. __C__ - __D__\n",
    "\n",
    "__Questions__:\n",
    "\n",
    "1. What is meant by \"The Gaussian distribution is fully characterized by its mean and covariance\"?\n",
    "2. If two Gaussians are added, can the (co-)variance ever decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answers__:\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPECTED SPACE FOR STUDENT CODE - START\n",
    "# gaussian_A = GaussianDistribution(mean=np.array([1]), cov=np.array([2]))\n",
    "\n",
    "# plot_gaussian(gaussian_A)\n",
    "# EXPECTED SPACE FOR STUDENT CODE - END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "__Task__: Implement the _marginalize_ function of the class GaussianDistribution. Test, for __C__, that your resulting Gaussian looks like the one in the plot. Plot the resulting distribution for __C__ when marginalizing on $y$, and on __D__ when marginalizing on $x$.\n",
    "\n",
    "__Question__: What does one effectively do when marginalizing a variable of a distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__:\n",
    "We remove the variable from consideration by summing out, or integrating, all possible values of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_marginals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code to test if you've marginalized distribution C correctly\n",
    "# marg_C = gaussian_C.marginalize(dim=1)\n",
    "\n",
    "# plot_gaussian(marg_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "__Task__: Implement the _get_conditional_ function of the class GaussianDistribution, as specified in Lecture 2 on Gaussians. Test, for the Gaussians __C__ and __D__ from 1.1, that your resulting conditional looks like the ones in the plots, when conditioning on $x=2$ and $y=4$, respectively. \n",
    "\n",
    "__Hint 1__: Use the numpy arrays to retrieve the right dimensions when performing the computation (_cond_dims_ are the dimensions we condition on - $y$ in the lecture notes, _non_cond_dims_ are the non-conditioned dimensions - $x$ in the lectures). Example:\n",
    "\n",
    "$\\Sigma_{yx} = $ self.cov[cond_dims, non_cond_dims][:, np.newaxis]\n",
    "\n",
    "__Hint 2__: Use either np.matmul, np.dot or the @ operator to perform matrix matrix multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that this produces the same plot (including the colors, which indicates the computed pdf)\n",
    "# gaussian_C = GaussianDistribution(mean=np.array([1, 0]), cov=np.array([[1, 0.9], [0.9, 2]]))\n",
    "\n",
    "# Use these lines to compute the conditionals\n",
    "# conditional_C = gaussian_C.get_conditional(dim=0, value=2)\n",
    "\n",
    "# Use these lines to generate the plots - plots both the initial distribution and the conditional along the right axis\n",
    "# plot_gaussian(gaussian_C, conditional_dist=conditional_C, condition_axis=0, condition_value=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Conjugate Priors\n",
    "\n",
    "Here, we will consider the example of flipping a coin. The coin may be biased, and has a probability $\\mu$ of heads. The probability of m number of heads in $N$ flips of a coin is a typical case of a *Binomial* distribution, with parameters $\\mu, N$.\n",
    "\n",
    "We would like to infer the probability $\\mu$ of getting heads. We know the following in advance:\n",
    "- A normal coin should have $\\mu=0.5$\n",
    "- The coin is weird in some way, so we cannot be sure that $\\mu=0.5$ \n",
    "\n",
    "To model this problem using Bayesian Inference, we want two things:\n",
    "- A prior probability distribution over $\\mu$ (our belief of $\\mu$ before seeing it)\n",
    "- A likelihood function - a probability distribution over our number of observed heads (which are *$Bin(m|N, \\mu$)*)\n",
    "\n",
    "In this case, the typical choice of prior probability distribution would be a *Beta* prior (with parameters $\\alpha$, $\\beta$), like the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_alpha, prior_beta = 3, 3\n",
    "pi_heads_prior = beta(prior_alpha, prior_beta)\n",
    "\n",
    "x = np.linspace(0, 1, 101)\n",
    "y = pi_heads_prior.pdf(x)\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.plot(x, y)\n",
    "ax.set_ylim(0)\n",
    "ax.set_title('Prior distribution over pi', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "A draw from the Beta distribution is always a number between 0 and 1, and represents our belief of the probability that we will get heads. In the example above, it says that you still think that the coin has $\\mu=0.5$ of heads (as that's where the probability mass is the largest).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "Toy around with the parameters of the beta distribution above. Approximately which parameters would indicate that\n",
    "\n",
    "a. You you think that $\\mu = 0.8$? (name one combination of alpha and beta)\n",
    "\n",
    "b. You you think that $\\mu = 0.2$? (name one combination of alpha and beta)\n",
    "\n",
    "c. You are almost certain that $\\mu = 0.5$?\n",
    "\n",
    "d. You genuinely have no idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "a.\n",
    "\n",
    "b.\n",
    "\n",
    "c.\n",
    "\n",
    "d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the choices of distributions above (*Beta* prior, *Binomial* likelihood), the *Beta* distributions is a **conjugate prior**. This means that the posterior distribution will be of the same form as the prior. As such, when we observe our coinflips, we will update the *Beta* distribution we started with to reflect what we have learned from our observations.\n",
    "\n",
    "**Task**: For the two coin flip datasets, compute the posterior over $\\mu$ using the prior in 2.1 and the updating described in Lecture 2 on Beta-Binomial conjugacy. This method of updating the posterior distribution in closed form is unique for conjugate priors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and change the coin index\n",
    "coin_index = 1\n",
    "coin_flips = pd.read_csv(f'data/coin_{coin_index}.csv')\n",
    "coin_flips.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "n_iterations = 100\n",
    "prior_alpha, prior_beta = 3, 3\n",
    "x = np.linspace(0, 1, 101)\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# plot prior before starting\n",
    "y = beta(prior_alpha, prior_beta).pdf(x)\n",
    "pl.plot(x, y)\n",
    "display.clear_output(wait=True)\n",
    "display.display(pl.gcf())\n",
    "\n",
    "posterior_alpha, posterior_beta = prior_alpha, prior_beta\n",
    "\n",
    "coin_flips_np = coin_flips.to_numpy()\n",
    "for i in range(n_iterations):\n",
    "    flip = coin_flips_np[i]\n",
    "    \n",
    "    # EXPECTED SPACE FOR STUDENT CODE - START\n",
    "\n",
    "    # here, you perform the posterior updating \n",
    "    \n",
    "    # EXPECTED SPACE FOR STUDENT CODE - END\n",
    "    \n",
    "    # clear the plot at regular intervals - comment out if no clearing\n",
    "    if i % 20 == 19:\n",
    "        pl.clf()\n",
    "        \n",
    "    # get the values of the posterior\n",
    "    y = posterior.pdf(x)\n",
    "    pl.plot(x, y)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(pl.gcf())\n",
    "    \n",
    "print('Final parameters of the distribution are\\nAlpha: {} Beta:{}'.format(posterior_alpha, posterior_beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "What posterior parameters $\\alpha$, $\\beta$ did you recieve for:\n",
    "\n",
    "a. Coin index 1 using 10 flips?\n",
    "\n",
    "b. Coin index 1 using 100 flips?\n",
    "\n",
    "c. Coin index 2 using 10 flips?\n",
    "\n",
    "d. Does the choice of prior matter for the the posterior distribution? When does it, and when does it not? Try for different priors and numbers of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "a.\n",
    "\n",
    "b.\n",
    "\n",
    "c.\n",
    "\n",
    "d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - MLE, Overfitting and Cross-validation\n",
    "Here, we are going to look at a synthetic regression problem, and how under- and overfitting can occur through better or worse model parameterizations. We will also look at the most common mean of evaluating model performance, namely cross-validation. We'll consider learning the function below through polynomial regression on a number of given data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(X):\n",
    "    return 12 * np.exp(-1e6 * np.power(1.2 * (X - 0.5), 16)) * np.sin((X - 0.5))\n",
    "\n",
    "def noisy_function(X):\n",
    "    return true_function(X) + np.random.randn(X.size) * 0.04\n",
    "\n",
    "eval_points = np.array([0.01, 0.1, 0.21, 0.52, 0.61, 0.78, 0.80, 0.81, 0.99])\n",
    "X_plot = np.linspace(0, 1, 101)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.plot(X_plot, true_function(X_plot), label='True function')\n",
    "ax.scatter(eval_points, noisy_function(eval_points), c='r', label='Training points')\n",
    "ax.legend(fontsize=16)\n",
    "ax.set_ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Look at the class _PolynomialRegression_ (a type of Generalized Linear Model) and pay attention to its attributes and _predict_ method. This class is intended to fit a polynomial of a given degree to the data points, by calling some method to fit the data points.\n",
    "\n",
    "__Task__: Implement the closed-form Maximum Likelihood parameter estimation in the _fit_MLE_ method. The closed-form expression for the optimal parameters can be found in Lecture 3, on the section on Maximum Likelihood. Ensure it works by fitting a polynomial of degree 10 to the data points - it should fit most data points reasonably well.\n",
    "\n",
    "__Hint__: Keep in mind that a polynomial of degree 2 has 3 coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegression:\n",
    "    \n",
    "    def __init__(self, degree):\n",
    "        \"\"\"\n",
    "        A Polynomial Regression model of a given degree,\n",
    "\n",
    "        Args:\n",
    "            degree (int): The degree of the polynomial regression.\n",
    "        \"\"\"        \n",
    "        self.degree = degree\n",
    "        self.weights = None\n",
    "        \n",
    "    def fit_MLE(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the weights of the model according to the Maximum Likelihood.\n",
    "\n",
    "        Args:\n",
    "            X ([type]): The explanatory variables of the data.\n",
    "            y ([type]): The dependent variables of the data.\n",
    "        \"\"\" \n",
    "        # EXPECTED SPACE FOR STUDENT CODE - START\n",
    "\n",
    "        # here, you fit the weights closed-form according to MLE\n",
    "        \n",
    "        # EXPECTED SPACE FOR STUDENT CODE - END\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output value(s) of a fitted model on the data point(s) X,\n",
    "\n",
    "        Args:\n",
    "            X ([type]): The explanatory variables of the data point(s) that is to \n",
    "            be predicted..\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no model is fitted (self.weights are None), no prediction\n",
    "            can be made and an error is raised.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The predicted output value of the data point(s) X.\n",
    "        \"\"\"        \n",
    "\n",
    "        if self.weights is None:\n",
    "            raise ValueError('Need to fit the model before predicting!')\n",
    "\n",
    "        theta = np.power(X[:, np.newaxis], np.arange(self.degree+1))\n",
    "        preds = theta.dot(self.weights)\n",
    "        return preds\n",
    "            \n",
    "    def print_weights(self):\n",
    "        \"\"\"\n",
    "        Prints the weights of the model in a structured format.\n",
    "        \"\"\"        \n",
    "        for i in range(self.degree):\n",
    "            print('Coeff. theta^{}:   {:.2f}'.format(i, self.weights[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_degree = 3\n",
    "\n",
    "\n",
    "pr = PolynomialRegression(polynomial_degree)\n",
    "\n",
    "X_eval = np.array([0.01, 0.05, 0.1, 0.21, 0.27, 0.32, 0.34, 0.52, 0.55, 0.68, 0.78, 0.80, 0.87, 0.97, 0.99])\n",
    "# calling noisy_function will sample new noise for each point!\n",
    "y_eval = noisy_function(X_eval)\n",
    "pr.fit_MLE(X_eval, y_eval)\n",
    "\n",
    "# Plotting\n",
    "X_plot = np.linspace(-0.0, 1.0, 101)\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.plot(X_plot, true_function(X_plot), label='True function')\n",
    "\n",
    "ax.scatter(X_eval, y_eval, c='r', label='Training points')\n",
    "ax.plot(X_plot, pr.predict(X_plot), label=f'Learned function of degree {polynomial_degree}')\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "We will now discover what happens if we under- and overfit parameters, as well as how additional data helps counteract overfitting. \n",
    "\n",
    "__Task__: Toy around with the fit of the generalized linear model below to find degrees where it under- and overfits. Try polynomials of different orders by changing the _polynomial_degree_ parameter. When you overfit, try change the _added_samples_ parameter.\n",
    "\n",
    "\n",
    "__Questions__:\n",
    "\n",
    "1. Judging by eye, which degree seems to consistently give the best fit?\n",
    "2. How can one tell, only by looking at the parameter values of the model, if overfitting has occured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__:\n",
    "\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out these parameters to answer the questions!\n",
    "polynomial_degree = 12\n",
    "added_samples = 30\n",
    "\n",
    "\n",
    "pr = PolynomialRegression(polynomial_degree)\n",
    "X_added = np.random.uniform(size=added_samples)\n",
    "# calling noisy_function will sample new noise for each point!\n",
    "y_eval = noisy_function(X_eval)\n",
    "y_added = noisy_function(X_added)\n",
    "\n",
    "all_X = np.append(X_eval, X_added, axis=0)\n",
    "all_y = np.append(y_eval, y_added, axis=0)\n",
    "\n",
    "pr.fit_MLE(all_X, all_y)\n",
    "pr.print_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = np.linspace(0, 1, 101)\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "\n",
    "\n",
    "ax.plot(X_plot, true_function(X_plot), label='True function')\n",
    "\n",
    "ax.scatter(X_eval, y_eval, c='r', label='Training points')\n",
    "if added_samples > 0:\n",
    "    ax.scatter(X_added, y_added, c='g', label='Additional points')\n",
    "ax.plot(X_plot, pr.predict(X_plot), label=f'Learned function of degree {polynomial_degree}')\n",
    "ax.set_ylim(-5, 5)\n",
    "ax.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "To evaluate the models produced, one commonly uses cross-validation. \n",
    "\n",
    "__Task__: Implement K-fold cross validation and find your best model with RMSE error as your metric. Plot the best model. Use only the initial data, without additional samples. Implement yourself or use an existing tool, e.g.  <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html\" title=\"Scikit-Learn Cross Validation\">Scikit-Learn</a>.\n",
    "\n",
    "__Tip__: Ensure nearby points don't go in the same batch, so that every model has data from the entire space.\n",
    "\n",
    "__Question__:\n",
    "What performance (RMSE, average on the test sets) did your top-performing model yield for K = 5?\n",
    "\n",
    "__Challenge__: Find a model with a cross-validation RMSE below __0.5__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPECTED SPACE FOR STUDENT CODE - START\n",
    "    \n",
    "# EXPECTED SPACE FOR STUDENT CODE - END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
